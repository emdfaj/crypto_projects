{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/emdfa/Downloads/cryptonews.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the first few rows\n",
    "#print(df.head())\n",
    "\n",
    "# Extract relevant columns\n",
    "df = df[['title', 'text', 'sentiment']]\n",
    "\n",
    "# Parse the 'sentiment' column to extract 'class' and 'polarity'\n",
    "import ast\n",
    "\n",
    "def parse_sentiment(sentiment_str):\n",
    "    sentiment_dict = ast.literal_eval(sentiment_str)\n",
    "    sentiment_class = sentiment_dict.get('class')\n",
    "    polarity = sentiment_dict.get('polarity')\n",
    "    return sentiment_class, polarity\n",
    "\n",
    "df[['sentiment_class', 'polarity']] = df['sentiment'].apply(parse_sentiment).apply(pd.Series)\n",
    "\n",
    "# Drop the original 'sentiment' column\n",
    "df = df.drop(columns=['sentiment'])\n",
    "\n",
    "# Map sentiment classes to numeric values (negative=0, neutral=1, positive=2)\n",
    "sentiment_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['sentiment_class'] = df['sentiment_class'].map(sentiment_mapping)\n",
    "\n",
    "# Check the processed dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40203c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocessing function to tokenize titles and texts\n",
    "def preprocess(text, max_len=128):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize the text and title for each row\n",
    "df['inputs'] = df['text'].apply(lambda x: preprocess(x))\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b27b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load the BERT model for classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create DataLoader for training data\n",
    "def create_data_loader(df, batch_size=8):\n",
    "    input_ids = torch.cat([x['input_ids'] for x in df['inputs']])\n",
    "    attention_masks = torch.cat([x['attention_mask'] for x in df['inputs']])\n",
    "    labels = torch.tensor(df['sentiment_class'].values)\n",
    "    \n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "train_loader = create_data_loader(train_df)\n",
    "test_loader = create_data_loader(test_df)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=b_input_ids,\n",
    "            attention_mask=b_attention_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == b_labels)\n",
    "\n",
    "    accuracy = correct_predictions.double() / len(train_df)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ce053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(\n",
    "                input_ids=b_input_ids,\n",
    "                attention_mask=b_attention_mask\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(b_labels.cpu().numpy())\n",
    "    \n",
    "    report = classification_report(all_labels, all_preds, target_names=['negative', 'neutral', 'positive'])\n",
    "    print(report)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text):\n",
    "    model.eval()\n",
    "    inputs = preprocess(text)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'].to(device),\n",
    "            attention_mask=inputs['attention_mask'].to(device)\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=1).cpu().item()\n",
    "    \n",
    "    polarity = ['negative', 'neutral', 'positive'][pred]\n",
    "    return polarity\n",
    "\n",
    "# Test on new headline\n",
    "new_headline = \"Bitcoin surges to new all-time high\"\n",
    "print(f\"Sentiment: {predict_sentiment(model, new_headline)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
